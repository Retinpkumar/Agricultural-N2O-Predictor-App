{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4f484b6",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d9cdd",
   "metadata": {},
   "source": [
    "### Feature Splitting and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5803aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce83c8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N2O</th>\n",
       "      <th>PP2</th>\n",
       "      <th>PP7</th>\n",
       "      <th>AirT</th>\n",
       "      <th>WFPS25cm</th>\n",
       "      <th>NH4</th>\n",
       "      <th>NO3</th>\n",
       "      <th>Mean_DAF</th>\n",
       "      <th>Season__Spring</th>\n",
       "      <th>Season__Summer</th>\n",
       "      <th>Season__Winter</th>\n",
       "      <th>Vegetation__Others</th>\n",
       "      <th>N_rate__1</th>\n",
       "      <th>Clay__2</th>\n",
       "      <th>Clay__3</th>\n",
       "      <th>Sand__2</th>\n",
       "      <th>Sand__3</th>\n",
       "      <th>SOM__1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.510578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.666508</td>\n",
       "      <td>2.488761</td>\n",
       "      <td>3.175585</td>\n",
       "      <td>5.558757</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.361374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.640608</td>\n",
       "      <td>2.485580</td>\n",
       "      <td>3.176368</td>\n",
       "      <td>5.562603</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.481391</td>\n",
       "      <td>1.458615</td>\n",
       "      <td>2.265921</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.728085</td>\n",
       "      <td>2.470780</td>\n",
       "      <td>3.187258</td>\n",
       "      <td>5.592851</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        N2O       PP2       PP7  AirT  WFPS25cm       NH4       NO3  Mean_DAF  \\\n",
       "0  2.510578  0.000000  0.000000  -2.0  0.666508  2.488761  3.175585  5.558757   \n",
       "1  2.361374  0.000000  0.000000  -2.4  0.640608  2.485580  3.176368  5.562603   \n",
       "2  2.481391  1.458615  2.265921   0.3  0.728085  2.470780  3.187258  5.592851   \n",
       "\n",
       "   Season__Spring  Season__Summer  Season__Winter  Vegetation__Others  \\\n",
       "0               0               0               1                   0   \n",
       "1               0               0               1                   0   \n",
       "2               0               0               1                   0   \n",
       "\n",
       "   N_rate__1  Clay__2  Clay__3  Sand__2  Sand__3  SOM__1  \n",
       "0          1        0        0        0        1       0  \n",
       "1          1        0        0        0        1       0  \n",
       "2          1        0        0        0        1       0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df = pd.read_csv('cleaned_data.csv')\n",
    "cleaned_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30298c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitData:\n",
    "    \"\"\"\n",
    "        Class for splitting dataset.\n",
    "        \n",
    "        Methods:\n",
    "        -------\n",
    "        * split_XY: For splitting input dataframe into dataframe of independant features and series of target feature\n",
    "        * split_TrainTest: For splitting features and label into training and testing datasets\n",
    "    \"\"\" \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def split_Xy(self):\n",
    "        \"\"\"\n",
    "            Function for seperating features and label\n",
    "            \n",
    "            Input:\n",
    "            ------\n",
    "            X : Dataframe for conversion\n",
    "            \n",
    "            Returns:\n",
    "            --------\n",
    "            self.X: Dataframe of input features\n",
    "            self.y: Series of target feature\n",
    "        \"\"\"\n",
    "        self.y = self.df['N2O'] # target\n",
    "        self.X = self.df.drop(columns=['N2O'], axis=1) # input features\n",
    "        return self.X, self.y\n",
    "    \n",
    "    def split_TrainTest(self, x_=None, y_=None, usevals=False, testsize:float = 0.25, randomstate:int=42):\n",
    "        \"\"\"\n",
    "            Function for splitting features and label into training and testing set\n",
    "            \n",
    "            Input:\n",
    "            ------\n",
    "            x_: Dataframe of input features\n",
    "            y_: Series of target feature\n",
    "            usevals: Boolean - True = Use X and y from split_Xy(), False = input X and y externally\n",
    "            testsize: Float - Default=0.25 ; Proportion of test data with respect to input dataset\n",
    "            random_state: Int - Default=42 ; Random seed state\n",
    "            \n",
    "            Returns:\n",
    "            --------\n",
    "            X_train: Training data of input features\n",
    "            X_test: Testing data of input features\n",
    "            y_train: Training data of target feature\n",
    "            y_test: Testing data of target feature\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        if usevals: # Using internal variables if set to True\n",
    "            x_ = self.X\n",
    "            y_ = self.y\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(x_, \n",
    "                                                            y_,\n",
    "                                                            test_size=testsize,\n",
    "                                                            random_state=randomstate)\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3357fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an object for splitting dataframe\n",
    "split = SplitData(cleaned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae798fc4",
   "metadata": {},
   "source": [
    "### Splitting data into feature dataframe and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6545e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split.split_Xy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13b0c4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2246, 17) (2246,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2da601",
   "metadata": {},
   "source": [
    "### Splitting data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d82e72f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split.split_TrainTest(usevals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a80d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1684, 17) (562, 17) (1684,) (562,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b6e714",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d98e3ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['standard_scalar.sav']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar = StandardScaler()\n",
    "\n",
    "# Scaling training data\n",
    "X_train_scaled = scalar.fit_transform(X_train)\n",
    "\n",
    "# Saving the scalar\n",
    "import joblib\n",
    "joblib.dump(scalar, 'standard_scalar.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04531c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling test data\n",
    "test_scalar = joblib.load('standard_scalar.sav')\n",
    "\n",
    "X_test_scaled = test_scalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afdff69",
   "metadata": {},
   "source": [
    "### Function for selecting best model among a list of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fde9a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To know the best baseline model\n",
    "def best_regression_model(models, X, y, nsplits=10, scoring ='neg_mean_squared_error'):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "        ------\n",
    "        models: list of tuples of model name and model.\n",
    "                eg:- models=[]\n",
    "                     models.append(('Linear Regression', LinearRegression()))\n",
    "                     models.append(('XG Boost', XGBoost()))\n",
    "        nsplits: Number of splits of KFold Cross validation; int, Default=10\n",
    "        scoring: Evaluation metric; Default='neg_mean_squared_error'\n",
    "        randomstate: Random seed state; int, Default=7\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        Prints: Model score mean and standard deviation\n",
    "        Boxplot: Model scores\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import cross_val_score, KFold\n",
    "    \n",
    "    results=[]\n",
    "    names = []\n",
    "\n",
    "    for name, model in models:\n",
    "\n",
    "        kfold = KFold(n_splits=nsplits)\n",
    "\n",
    "        cv_results = cross_val_score(model, \n",
    "                                     X, y, \n",
    "                                     cv=kfold, \n",
    "                                     scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "\n",
    "        names.append(name)\n",
    "\n",
    "        msg = f\"{name} >> Average score: {round(cv_results.mean(),5)}\"\n",
    "        print(msg)\n",
    "\n",
    "    # boxplot of models for comparison\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    fig.suptitle('Algorithm Comparison')\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(results, vert=False, showmeans=True)\n",
    "    ax.set_yticklabels(names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "598dfbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feeding some base models\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "models = []\n",
    "models.append(('KNN', KNeighborsRegressor()))\n",
    "models.append(('SVR', SVR()))\n",
    "models.append(('RF', RandomForestRegressor()))\n",
    "models.append(('GB', GradientBoostingRegressor()))\n",
    "models.append(('XGB', XGBRegressor()))\n",
    "models.append(('LGBM', LGBMRegressor()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea3163a",
   "metadata": {},
   "source": [
    "### Finding the best baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4369c8dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN >> Average score: -0.07727\n",
      "SVR >> Average score: -0.07324\n",
      "RF >> Average score: -0.05725\n",
      "GB >> Average score: -0.06795\n",
      "XGB >> Average score: -0.06313\n",
      "LGBM >> Average score: -0.05748\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Finding the best baseline models\n",
    "best_regression_model(models, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef1ba97",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "* KNN and SVR are performing really bad even after scaling.\n",
    "* LGBMRegressor and RandomForestRegressor are the top performing models with respect to mean scores. \n",
    "* But, we find that even with a bigger range of values, smaller IQR and lower Q3 values, LGBMRegressor has produced maximum scores among all models and tends to be much more stable than RandomForestRegressor when comparing median values.\n",
    "* Therefore, we will use LGBMRegressor as our initial base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cec828",
   "metadata": {},
   "source": [
    "### Initial Prediction using a base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7a3503f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:  [2.10827005 2.104714   2.23714142 2.40831305 2.51152678 2.31448405\n",
      " 2.21674641 2.17778086 2.20325075 2.75166329 2.73498274 2.23355498\n",
      " 2.95224381 2.40448555 2.41690132 2.69585948 2.37310036 2.17912489\n",
      " 2.6995435  2.2799985  3.25827287 2.25775826 3.06445127 2.18514019\n",
      " 2.20898528 2.83340633 2.6876075  3.14484372 2.34693404 2.4798206\n",
      " 2.32155485 2.2553786  2.92246918 2.12819281 2.36105591 2.90258197\n",
      " 3.26641741 2.94763369 2.44724015 2.21666109 2.31764779 2.25073825\n",
      " 2.09409121 2.76639591 2.48677678 2.49116696 2.23283027 3.06904413\n",
      " 2.16631818 2.29686357 2.76093014 2.98963155 2.43614893 2.72859055\n",
      " 2.53394538 2.42925537 2.45366195 2.74768673 3.04381502 2.31382898\n",
      " 2.20653018 2.17813953 2.29252062 2.47656595 3.00137918 2.40238071\n",
      " 2.58257672 2.82553795 2.03719488 2.45943505 2.55450579 2.3225927\n",
      " 1.87151765 2.36740131 2.58443117 2.54197409 2.35260392 2.67383038\n",
      " 2.33897557 2.57934645 2.32626067 2.44498076 2.3716144  2.32776973\n",
      " 2.57192393 2.20944915 2.71056067 2.55661556 2.54304377 2.26478916\n",
      " 2.63154616 2.4841634  2.2906536  2.17928895 2.98696514 2.13693815\n",
      " 2.05239185 2.40645507 2.32341135 2.29495247 2.31660778 2.20729932\n",
      " 2.62678135 2.49422721 2.519774   2.28225385 2.30645144 2.31094108\n",
      " 2.24537218 3.27691001 2.13776126 2.42430472 2.63207426 3.13493565\n",
      " 2.38121421 2.24395923 2.47005228 3.24908775 2.38467768 2.39813499\n",
      " 2.76080561 2.88440596 2.38753547 2.3966496  2.20511172 2.66782892\n",
      " 2.23044295 2.60967126 2.2179341  2.45148894 2.31885772 2.08537869\n",
      " 2.32017929 2.21711422 2.14809378 2.55342019 2.07665195 2.25119861\n",
      " 2.37536974 2.2142637  2.27331278 3.00874555 2.33911746 2.16970399\n",
      " 2.48329873 2.3960863  2.67425446 2.49460813 3.06288291 2.26478375\n",
      " 2.13917446 2.63667459 2.60729457 2.3063059  2.51347759 2.34746203\n",
      " 3.09335409 2.19499475 2.3860659  2.1967652  2.45731518 2.37970352\n",
      " 2.25267829 2.68916794 2.40376331 2.38105095 2.15820446 2.43583476\n",
      " 2.12634218 2.16398604 2.29587788 2.65853967 2.71177803 2.18240987\n",
      " 2.29808613 2.26518265 2.1415577  2.13494143 2.38801213 2.35202945\n",
      " 2.33162343 2.51770542 2.77919777 2.0872524  2.66469277 2.35082048\n",
      " 2.58722387 2.17160152 2.37414349 2.17392869 2.24847591 2.64595591\n",
      " 3.16946812 2.64024298 2.24556491 2.45334028 2.24602677 3.05419766\n",
      " 2.305894   2.84440194 2.20074501 2.81152593 2.27039632 2.88137874\n",
      " 2.46543345 2.3040809  2.91184996 2.25047219 3.09702811 2.63988368\n",
      " 2.17753354 2.30357118 2.87502906 2.36396383 2.39502048 2.33347295\n",
      " 2.89660089 2.44309246 2.10502506 2.22909912 2.52273655 2.14603683\n",
      " 2.13798181 2.17704144 2.15731635 2.32481829 2.75810516 2.45862711\n",
      " 2.57431048 2.13361447 2.38354999 2.35923539 2.16337624 2.34153188\n",
      " 2.27847403 3.29807913 2.89233659 2.11848142 2.34859478 2.42507381\n",
      " 2.34161136 2.21796522 2.30618561 2.28545297 2.27443389 1.83969791\n",
      " 2.14171811 2.59681221 2.42684048 2.19584787 2.17747371 2.21732929\n",
      " 2.68047497 2.55077627 2.18124841 2.25431161 2.14413608 2.22823528\n",
      " 2.88908388 2.06432136 2.18839563 3.04057118 2.08210082 2.47968571\n",
      " 2.68829918 2.12605849 2.22727982 2.02109442 2.2534053  2.44513221\n",
      " 3.1005196  2.2224465  2.35167699 2.71229151 2.3715305  2.08490755\n",
      " 3.12526777 2.32568517 2.46664862 2.9434619  2.24379395 2.33791743\n",
      " 3.27501949 2.17280577 2.71696391 2.34423785 2.47642594 2.37943865\n",
      " 2.52873809 2.25040673 2.96763753 2.65730788 2.83901121 2.40222439\n",
      " 2.38185718 2.36643849 2.30694226 2.53158232 2.56439487 2.24460909\n",
      " 2.89841685 2.19625266 2.21491456 2.21626892 2.34017974 2.25249096\n",
      " 2.20706815 2.19876793 2.44204343 2.25800082 2.10624166 2.66459051\n",
      " 2.14511129 2.29151598 2.51795894 2.1055156  2.65299775 2.14490184\n",
      " 2.61212937 2.26172916 2.24330008 2.40193151 2.33341956 2.44835178\n",
      " 2.64822361 3.04519821 2.52293923 2.67169793 2.24649936 3.20925671\n",
      " 2.32763995 2.63207426 2.12629445 2.43535565 2.28136315 2.99864612\n",
      " 2.1514271  2.3282716  1.80790326 2.30137821 2.36617673 2.76200042\n",
      " 2.20063454 2.84029976 2.60145787 2.31071553 2.21728586 3.10554523\n",
      " 2.21829512 2.37481196 2.25229632 2.29105672 2.21872749 2.52726825\n",
      " 2.26316319 2.49745967 2.33897557 2.2707024  2.59328583 2.50154262\n",
      " 2.51004989 2.2245072  2.55853939 2.75612882 3.03069459 2.39694581\n",
      " 2.44879441 2.26091554 2.38612303 2.55783527 2.50235629 2.59306875\n",
      " 2.50557893 2.32055674 2.1814779  2.45676151 2.55402205 2.58256561\n",
      " 2.48731285 2.3447877  2.30773592 2.35027572 2.45154343 2.70180419\n",
      " 2.40014962 2.39216301 2.05204032 2.33252087 2.55666114 2.21405296\n",
      " 2.8072693  2.28725316 2.19982115 2.34943301 2.66194411 2.11025671\n",
      " 2.45387532 2.28173255 2.21828937 2.13985071 2.12531014 2.52231387\n",
      " 2.27942103 2.52411503 2.08083737 2.6729414  2.53502122 2.38987394\n",
      " 2.12360425 2.18147532 2.8191221  2.3831645  2.25337311 2.72393322\n",
      " 2.3646597  2.0992879  2.28053003 2.37015113 2.2456686  2.35642343\n",
      " 2.3176805  3.04583126 2.49074189 2.89630247 3.00581564 1.95443613\n",
      " 2.17901805 2.25620565 2.21005073 3.0466195  2.5831239  2.44734557\n",
      " 2.12249273 2.23104564 2.5384094  2.63459325 2.3161205  2.19267\n",
      " 2.39052698 2.21348492 2.57121133 2.33994601 2.35250575 2.63176256\n",
      " 2.78217974 2.41239504 2.31180582 2.24649124 2.55546489 2.16428906\n",
      " 2.28795107 2.25194594 2.4723062  2.29010398 2.26725564 2.45774285\n",
      " 2.20934841 2.94163887 3.15505763 2.71567776 2.70776678 3.03295338\n",
      " 2.25143406 2.2424027  2.25556517 2.17484452 2.37780548 2.48769178\n",
      " 2.82214027 3.02534206 2.22221493 2.80354434 2.12814262 2.35558242\n",
      " 2.3663162  2.44941763 2.79609333 2.93841036 2.32516996 2.79239197\n",
      " 1.77407299 2.6989478  2.17114797 2.21770755 2.2293101  2.29299166\n",
      " 2.37996299 2.60244115 2.73910066 2.17018195 2.32843979 2.48251462\n",
      " 2.76076729 2.24576113 2.84491683 2.3587721  2.53921577 2.62346763\n",
      " 2.24877196 2.94810634 2.35148319 2.26190328 2.24560549 2.27258274\n",
      " 2.63853245 2.49349117 2.4116256  2.37886652 2.56973708 2.38667454\n",
      " 2.57238108 2.46911766 2.15455305 2.58713914 2.51199645 3.11231289\n",
      " 2.29609529 2.12112986 2.34287728 2.31317542 2.36689603 2.4521931\n",
      " 2.32685443 2.36742904 2.33540368 2.28333659 2.18512051 2.26923894\n",
      " 2.68097604 2.51460447 2.91963477 2.2549181  2.17057057 2.30444571\n",
      " 2.7892909  2.43406446 2.6153271  1.98587512 2.30496084 2.37852404\n",
      " 2.44576397 2.55316408 2.63176256 2.34258729 2.45996611 2.63933474\n",
      " 2.61223787 2.37202657 2.2442462  2.32837713 2.67661677 2.3797029\n",
      " 2.19572028 2.81292461 2.4562642  2.33453207 2.3144963  2.34351692\n",
      " 2.44171702 2.92753329 2.42317437 2.52189999] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Random Forest Regressor\n",
    "base_model = LGBMRegressor()\n",
    "base_model.fit(X_train_scaled, y_train) # model fitting\n",
    " \n",
    "# Predicting the Target variable\n",
    "y_pred = base_model.predict(X_test_scaled)\n",
    "print(\"Predicted values: \", y_pred, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a10765f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.59889\n",
      "Adjusted R2:  0.58635\n"
     ]
    }
   ],
   "source": [
    "# Printing model score\n",
    "r2 = base_model.score(X_test_scaled, y_test)\n",
    "print(\"R2: \",round(r2,5))\n",
    "\n",
    "# Function to calculate adjusted R2\n",
    "def adj_r2(x,y,model):\n",
    "    r2 = model.score(x,y) # calculating r2\n",
    "    n = x.shape[0] # observations in dataset\n",
    "    p = x.shape[1] # features in dataset\n",
    "    adjusted_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "    return adjusted_r2\n",
    "\n",
    "adjustedR2 = adj_r2(X_test_scaled, y_test, base_model)\n",
    "print(\"Adjusted R2: \", round(adjustedR2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8bf17b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on training data:  0.87088\n",
      "Adjusted R2 on training data:  0.86957\n",
      "MSE:  0.049952963976120784\n"
     ]
    }
   ],
   "source": [
    "# Checking how the model fits the training data\n",
    "print(\"R2 on training data: \", round(base_model.score(X_train_scaled, y_train), 5))\n",
    "print(\"Adjusted R2 on training data: \", round(adj_r2(X_train_scaled, y_train, base_model),5),)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"MSE: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0667a295",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "* Our best base model overfits the training data by a huge amount. \n",
    "* There is also a room for improvement in fitting the data properly. Even the MSE value for the prediction is lower than best value obtained during model comparison.\n",
    "* Hence, we will require further optimizations inlcuding hyperparameter tuning to fine tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4777e676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base_model.sav']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the base model to create a minimum viable version of streamlit app\n",
    "joblib.dump(base_model, 'base_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "097191bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e154ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "880de776",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9425d",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c2c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# # Create Study object\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# # Optimize the study, use more trials to obtain better result, use less trials to be more cost-efficient\n",
    "# study.optimize(objective, n_trials=100) # Use more \n",
    "# # Print the result\n",
    "# best_params = study.best_params\n",
    "# best_score = study.best_value\n",
    "# print(f\"Best score: {best_score}\\n\")\n",
    "# print(f\"Optimized parameters: {best_params}\\n\")\n",
    "\n",
    "# X---------X---------------X-------------------X------------------X-------------------------X-----------------X\n",
    "\n",
    "# RANDOM_SEED = 42\n",
    "\n",
    "# # 10-fold CV\n",
    "# kfolds = KFold(n_splits=10, shuffle=True, random_state=RANDOM_SEED)\n",
    "# # Define the helper function so that it can be reused\n",
    "# def tune(objective):\n",
    "#     study = optuna.create_study(direction=\"maximize\")\n",
    "#     study.optimize(objective, n_trials=100)\n",
    "\n",
    "#     params = study.best_params\n",
    "#     best_score = study.best_value\n",
    "#     print(f\"Best score: {best_score}\\n\")\n",
    "#     print(f\"Optimized parameters: {params}\\n\")\n",
    "#     return params\n",
    "# ##################\n",
    "# # Ridge\n",
    "# ##################\n",
    "# def ridge_objective(trial):\n",
    "#     _alpha = trial.suggest_float(\"alpha\", 0.1, 20)\n",
    "#     ridge = Ridge(alpha=_alpha, random_state=RANDOM_SEED)\n",
    "#     scores = cross_val_score(\n",
    "#         ridge, X, y, cv=kfolds,\n",
    "#         scoring=\"neg_root_mean_squared_error\"\n",
    "#     )\n",
    "#     return scores.mean()\n",
    "\n",
    "# ridge_params = tune(ridge_objective)\n",
    "# # After tuning it for once, we can copy the best params to create the model without tunning it again\n",
    "# # ridge_params = {'alpha': 7.491061624529043}\n",
    "# ridge = Ridge(**ridge_params, random_state=RANDOM_SEED)\n",
    "# ##################\n",
    "# # Lasso\n",
    "# ##################\n",
    "# def lasso_objective(trial):\n",
    "#     _alpha = trial.suggest_float(\"alpha\", 0.0001, 1)\n",
    "#     lasso = Lasso(alpha=_alpha, random_state=RANDOM_SEED)\n",
    "#     scores = cross_val_score(\n",
    "#         lasso, X, y, cv=kfolds,\n",
    "#         scoring=\"neg_root_mean_squared_error\"\n",
    "#     )\n",
    "#     return scores.mean()\n",
    "# lasso_params = tune(lasso_objective)\n",
    "# # lasso_params = {'alpha': 0.00041398687418613947}\n",
    "# lasso = Lasso(**lasso_params, random_state=RANDOM_SEED)\n",
    "# ##################\n",
    "# # Random Forest\n",
    "# ##################\n",
    "# def randomforest_objective(trial):\n",
    "#     _n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n",
    "#     _max_depth = trial.suggest_int(\"max_depth\", 5, 20)\n",
    "#     _min_samp_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "#     _min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 2, 10)\n",
    "#     _max_features = trial.suggest_int(\"max_features\", 10, 50)\n",
    "\n",
    "#     rf = RandomForestRegressor(\n",
    "#         max_depth=_max_depth,\n",
    "#         min_samples_split=_min_samp_split,\n",
    "#         min_samples_leaf=_min_samples_leaf,\n",
    "#         max_features=_max_features,\n",
    "#         n_estimators=_n_estimators,\n",
    "#         n_jobs=-1,\n",
    "#         random_state=RANDOM_SEED,\n",
    "#     )\n",
    "\n",
    "#     scores = cross_val_score(\n",
    "#         rf, X, y, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n",
    "#     )\n",
    "#     return scores.mean()\n",
    "\n",
    "# randomforest_params = tune(randomforest_objective)\n",
    "# # randomforest_params = {'n_estimators': 180, 'max_depth': 18, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 49}\n",
    "# rf = RandomForestRegressor(n_jobs=-1, random_state=RANDOM_SEED, **randomforest_params)\n",
    "# # So on with other models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b821e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
